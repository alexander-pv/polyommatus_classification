{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac52f2ad-5f65-4349-b3e1-f08e5f21cbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import v2 as transforms_v2 \n",
    "\n",
    "from pytorch_grad_cam import GradCAM, GradCAMPlusPlus\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209d2b98-e0d3-4001-a845-3524bb0b1c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ee782b-be40-4f19-824f-897b350df42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.classifier import MODELS, get_pretrained_model\n",
    "from src.dataset.dataset import LycaenidaeDatasetCls\n",
    "from src.dataset.transform import get_cls_pretrained_transform\n",
    "from src.utils import get_best_model_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dc63c6-1894-4c18-9675-10345c52e2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gradcam(\n",
    "    dataset: LycaenidaeDatasetCls,\n",
    "    model: nn.Module,\n",
    "    target_layers: list[nn.Module],\n",
    "    resize: tuple[int, int],\n",
    "    img_idx: int,\n",
    "    save_dir: str,\n",
    "    interpolation: int = InterpolationMode.BILINEAR,\n",
    "    reshape_transform: callable or None = None\n",
    "):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    img, label, img_path = dataset._getitem(img_idx)\n",
    "    filename = img_path.split(os.sep)[-1]\n",
    "    input_tensor, _ = dataset.__getitem__(img_idx)\n",
    "    \n",
    "    input_tensor = torch.unsqueeze(input_tensor, 0)\n",
    "\n",
    "    preproc_img = torch.as_tensor(img, dtype=torch.float32) \n",
    "    preproc_img = preproc_img.permute((2, 0, 1)).contiguous()\n",
    "    preproc_img = F.resize(preproc_img, resize, interpolation=interpolation).permute(1, 2, 0).numpy().astype(np.uint8) / 255\n",
    "\n",
    "\n",
    "    targets = [ClassifierOutputTarget(label)]\n",
    "\n",
    "    with GradCAM(model=model, target_layers=target_layers, reshape_transform=reshape_transform) as cam:\n",
    "        grayscale_cams = cam(input_tensor=input_tensor, targets=targets)\n",
    "        cam_image = show_cam_on_image(preproc_img, grayscale_cams[0, :], use_rgb=True)\n",
    "        \n",
    "    with GradCAMPlusPlus(model=model, target_layers=target_layers, reshape_transform=reshape_transform) as campp:\n",
    "        grayscale_campps = campp(input_tensor=input_tensor, targets=targets)\n",
    "        campp_image = show_cam_on_image(preproc_img, grayscale_campps[0, :], use_rgb=True)\n",
    "\n",
    "    cam = np.uint8( 255 * grayscale_cams[0, :])\n",
    "    campp = np.uint8( 255 * grayscale_campps[0, :])\n",
    "    images = {\n",
    "        \"cam\": (np.uint8(255 * preproc_img), cv2.merge([cam, cam, cam]) , cam_image),\n",
    "        \"campp\": (np.uint8(255 * preproc_img), cv2.merge([campp, campp, campp]) , campp_image),\n",
    "    }\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=3, figsize= (24, 12))\n",
    "    \n",
    "    for idx, cam_name in enumerate(images.keys()):\n",
    "        for ax, img_to_plot in zip(axes[idx], images[cam_name]):\n",
    "            ax.imshow(img_to_plot)\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "    class_name = dataset.class_labels_inv[label]\n",
    "    axes[0][0].set_title(f\"{class_name}. {filename}\")\n",
    "    axes[0][1].set_title(\"GradCAM, GradCAM++\")\n",
    "    axes[0][2].set_title(\"Image & GradCAM, GradCAM++\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, f\"gradcam_{class_name}_{filename.split('.')[0]}.jpg\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b21af7-0cc9-4a8c-9bd4-e00977cd661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "METADATA_PATH = \"./data/meta_all_groups_v3.csv\"\n",
    "GRADCAM_DIR = \"./gradcam_output\"\n",
    "\n",
    "train_size = 0.8\n",
    "test_size = 0.1\n",
    "val_size = 0.1\n",
    "min_images_per_class = 5\n",
    "seed = 42\n",
    "device = \"cuda:0\"\n",
    "os.makedirs(GRADCAM_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8eba99-3494-4dee-aeb4-8abe1d44b5e7",
   "metadata": {},
   "source": [
    "#### Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc719543-3599-4063-a0e0-b712bf446287",
   "metadata": {},
   "outputs": [],
   "source": [
    "for view in (\"top\", \"bottom\"):\n",
    "    model_name = \"resnet50\"\n",
    "    weights_dir = f\"./weights_cls/ckpt_all_groups_cbalanced_sampler_upd_26-07-2025_view-{view}_{model_name}\"\n",
    "    gradcam_dir = os.path.join(GRADCAM_DIR, model_name, view)\n",
    "    resnet50_transforms_no_crop = transforms.Compose(\n",
    "        [\n",
    "        transforms_v2.Resize((232, 232)),    \n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ]\n",
    "    )\n",
    "    test_set = LycaenidaeDatasetCls(\n",
    "        metadata_path=METADATA_PATH,\n",
    "        view=view,\n",
    "        subset=\"test\",\n",
    "        train_size=0.8,\n",
    "        test_size=0.1,\n",
    "        val_size=0.1,\n",
    "        transform=None,\n",
    "        model_transform=resnet50_transforms_no_crop,\n",
    "        min_images_per_class=min_images_per_class,\n",
    "        device=device,\n",
    "        seed=seed\n",
    "    )\n",
    "    resnet50 = get_pretrained_model(name=model_name, n_classes=test_set.n_classes)\n",
    "    best_weights_path = get_best_model_weights(dirname=weights_dir, score_name=\"inv_loss\")\n",
    "    print(f\"Best weights: {best_weights_path}\")\n",
    "\n",
    "    resnet50.load_state_dict(torch.load(best_weights_path))\n",
    "    resnet50 = resnet50.to(device)\n",
    "    resnet50 = resnet50.eval()\n",
    "\n",
    "    for idx in tqdm.tqdm(range(test_set.__len__())):\n",
    "        plot_gradcam(\n",
    "            test_set,\n",
    "            resnet50,\n",
    "            [resnet50.layer1, resnet50.layer2, resnet50.layer3, resnet50.layer4],\n",
    "            (232, 232),\n",
    "            idx,\n",
    "            gradcam_dir\n",
    "        )\n",
    "    with open(os.path.join(gradcam_dir, \"info.txt\"), \"w\") as f:\n",
    "        f.write(f\"\\nModel weights: {best_weights_path}\")\n",
    "        f.write(\"\\nSelected layers: resnet50_bot.layer1, resnet50_bot.layer2, resnet50_bot.layer3, resnet50_bot.layer4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a139ae16-5a59-4070-b6ae-8876bfa8ca24",
   "metadata": {},
   "outputs": [],
   "source": [
    "del resnet50\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e5824c-3737-40af-bef9-15078d3c3214",
   "metadata": {},
   "source": [
    "#### MobileNetV3Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4611dbd1-2afe-4334-8c15-8203ee9c10c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for view in (\"top\", \"bottom\"):\n",
    "    model_name = \"mobilenet_v3_large\"\n",
    "    weights_dir = f\"./weights_cls/ckpt_all_groups_cbalanced_sampler_upd_26-07-2025_view-{view}_{model_name}\"\n",
    "    gradcam_dir = os.path.join(GRADCAM_DIR, model_name, view)\n",
    "    mmbnet_transforms_no_crop = transforms.Compose(\n",
    "        [\n",
    "        transforms_v2.Resize((232, 232)),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ]\n",
    "    )\n",
    "    test_set = LycaenidaeDatasetCls(\n",
    "        metadata_path=METADATA_PATH,\n",
    "        view=view,\n",
    "        subset=\"test\",\n",
    "        train_size=0.8,\n",
    "        test_size=0.1,\n",
    "        val_size=0.1,\n",
    "        transform=None,\n",
    "        model_transform=mmbnet_transforms_no_crop,\n",
    "        min_images_per_class=min_images_per_class,\n",
    "        device=device,\n",
    "        seed=seed\n",
    "    )\n",
    "    mbnet = get_pretrained_model(name=model_name, n_classes=test_set.n_classes)\n",
    "    best_weights_path = get_best_model_weights(dirname=weights_dir, score_name=\"inv_loss\")\n",
    "    print(f\"Best weights: {best_weights_path}\")\n",
    "\n",
    "    mbnet.load_state_dict(torch.load(best_weights_path))\n",
    "    mbnet = mbnet.to(device)\n",
    "    mbnet = mbnet.eval()\n",
    "\n",
    "    for idx in tqdm.tqdm(range(test_set.__len__())):\n",
    "        plot_gradcam(\n",
    "            test_set,\n",
    "            mbnet,\n",
    "            [mbnet.features[0], mbnet.features[5], mbnet.features[10], mbnet.features[15]],\n",
    "            (232, 232),\n",
    "            idx,\n",
    "            gradcam_dir\n",
    "        )\n",
    "    with open(os.path.join(gradcam_dir, \"info.txt\"), \"w\") as f:\n",
    "        f.write(f\"\\nModel weights: {best_weights_path}\")\n",
    "        f.write(\"\\nSelected layers: mbnet.features[0], mbnet.features[5], mbnet.features[10], mbnet.features[15]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9a7bad-8371-4b7e-95cb-043395a2c3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "del mbnet\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89ebba1-e9e8-48e7-80b1-19e50e2d837e",
   "metadata": {},
   "source": [
    "#### EfficientNetV2L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a286e1f-acfe-4781-93ac-b11882f58164",
   "metadata": {},
   "outputs": [],
   "source": [
    "for view in (\"top\", \"bottom\"):\n",
    "    model_name = \"efficientnet_v2_l\"\n",
    "    weights_dir = f\"./weights_cls/ckpt_all_groups_cbalanced_sampler_upd_26-07-2025_view-{view}_{model_name}\"\n",
    "    gradcam_dir = os.path.join(GRADCAM_DIR, model_name, view)\n",
    "    effnetv2l_transforms_no_crop = transforms.Compose(\n",
    "        [\n",
    "        transforms_v2.Resize((480, 480)),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ]\n",
    "    )\n",
    "    test_set = LycaenidaeDatasetCls(\n",
    "        metadata_path=METADATA_PATH,\n",
    "        view=view,\n",
    "        subset=\"test\",\n",
    "        train_size=0.8,\n",
    "        test_size=0.1,\n",
    "        val_size=0.1,\n",
    "        transform=None,\n",
    "        model_transform=effnetv2l_transforms_no_crop,\n",
    "        min_images_per_class=min_images_per_class,\n",
    "        device=device,\n",
    "        seed=seed\n",
    "    )\n",
    "    effnet = get_pretrained_model(name=model_name, n_classes=test_set.n_classes)\n",
    "    best_weights_path = get_best_model_weights(dirname=weights_dir, score_name=\"inv_loss\")\n",
    "    print(f\"Best weights: {best_weights_path}\")\n",
    "\n",
    "    effnet.load_state_dict(torch.load(best_weights_path))\n",
    "    effnet = effnet.to(device)\n",
    "    effnet = effnet.eval()\n",
    "\n",
    "    for idx in tqdm.tqdm(range(test_set.__len__())):\n",
    "        plot_gradcam(\n",
    "            test_set,\n",
    "            effnet,\n",
    "            [effnet.features[6], effnet.features[7], effnet.features[8]],\n",
    "            (480, 480),\n",
    "            idx,\n",
    "            gradcam_dir\n",
    "        )\n",
    "    with open(os.path.join(gradcam_dir, \"info.txt\"), \"w\") as f:\n",
    "        f.write(f\"\\nModel weights: {best_weights_path}\")\n",
    "        f.write(\"\\nSelected layers: mbnet.features[0], mbnet.features[5], mbnet.features[10], mbnet.features[15]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faca0ef2-6550-4af4-8786-7cba6c891391",
   "metadata": {},
   "outputs": [],
   "source": [
    "del effnet\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84f68c3-5eb5-439b-850e-174a34ac18db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e2a2eb-39ee-4793-8c2b-4c300e435264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
